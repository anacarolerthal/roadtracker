{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, countDistinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    mongo_conn = \"mongodb://mongo:27017\"\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\")\n",
    "    conf.set(\"spark.write.connection.uri\", mongo_conn)\n",
    "    conf.set(\"spark.mongodb.write.database\", \"roadtracker\")\n",
    "    conf.set(\"spark.mongodb.write.collection\", \"historical1\")\n",
    "    \n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    \n",
    "    return SparkSession(sc) \\\n",
    "        .builder \\\n",
    "        .appName(\"RoadTracker\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/20 16:39:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = spark.read.csv(\"all_roads.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# read a parquet file\n",
    "# df = spark.read.parquet(\"1686522436.8648.parquet\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|plate|road_count|\n",
      "+-----+----------+\n",
      "|SF428|         1|\n",
      "|TW315|         1|\n",
      "|JL736|         1|\n",
      "|EP171|         1|\n",
      "|PB094|         1|\n",
      "|MD954|         1|\n",
      "|ZZ212|         1|\n",
      "|ZP226|         1|\n",
      "|UN926|         1|\n",
      "|GF380|         1|\n",
      "|DT049|         1|\n",
      "|LI295|         1|\n",
      "|GB081|         1|\n",
      "|NP659|         1|\n",
      "|BK572|         1|\n",
      "|HI030|         1|\n",
      "|VF070|         1|\n",
      "|OG955|         1|\n",
      "|UX361|         1|\n",
      "|ZP642|         1|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o716.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m dfRoadCount \u001b[39m=\u001b[39m dfRoadCount\u001b[39m.\u001b[39morderBy(col(\u001b[39m\"\u001b[39m\u001b[39mroad_count\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdesc())\u001b[39m.\u001b[39mlimit(\u001b[39m100\u001b[39m)\n\u001b[1;32m      6\u001b[0m dfRoadCount\u001b[39m.\u001b[39mshow()\n\u001b[1;32m      8\u001b[0m dfRoadCount\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mmongodb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      9\u001b[0m    \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     10\u001b[0m    \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdatabase\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mroadtracker\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     11\u001b[0m    \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mcollection\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhistorical1\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m---> 12\u001b[0m    \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[1;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o716.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "# historical 1\n",
    "dfRoadCount = df.groupBy(\"plate\").agg(countDistinct('road')).withColumnRenamed(\"count(road)\", \"road_count\")\n",
    "\n",
    "# get the top 100\n",
    "dfRoadCount = dfRoadCount.orderBy(col(\"road_count\").desc()).limit(100)\n",
    "dfRoadCount.show()\n",
    "\n",
    "dfRoadCount.write.format(\"mongodb\") \\\n",
    "   .mode(\"append\") \\\n",
    "   .option(\"database\", \"roadtracker\") \\\n",
    "   .option(\"collection\", \"historical1\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+---+---+-----+--------------------+---------+-----+---+\n",
      "| road|road_speed|road_size|  x|  y|plate|                time|direction|speed|acc|\n",
      "+-----+----------+---------+---+---+-----+--------------------+---------+-----+---+\n",
      "|road3|       120|     1000|220|  2|AC272| 1.686422274283066E9|        1|    0|  0|\n",
      "|road3|       120|     1000|220|  2|AC272|1.6864222536890728E9|        1|    0|  0|\n",
      "|road3|       120|     1000|220|  2|AC272| 1.686422231388323E9|        1|    0|  0|\n",
      "|road3|       120|     1000|220|  2|AC272|1.6864222095374138E9|        1|    0|  0|\n",
      "|road3|       120|     1000|220|  2|AC272|  1.68642220938139E9|        1|    0|  0|\n",
      "|road3|       120|     1000|220|  2|AC272|1.6864222092032158E9|        1|    0|-35|\n",
      "|road3|       120|     1000|220|  2|AC272|1.6864222087273128E9|        1|   35|-26|\n",
      "|road3|       120|     1000|185|  2|AC272|1.6864221860634181E9|        1|   61| -1|\n",
      "|road3|       120|     1000|331|  9|AU218| 1.686422286230809E9|       -1|   72|  2|\n",
      "|road3|       120|     1000|403|  9|AU218| 1.686422263140699E9|       -1|   70|  0|\n",
      "|road3|       120|     1000|473|  9|AU218|1.6864222420804741E9|       -1|   70|  0|\n",
      "|road3|       120|     1000|543|  9|AU218| 1.686422219653634E9|       -1|   70| -2|\n",
      "|road3|       120|     1000|613|  9|AU218| 1.686422197781182E9|       -1|   72| -5|\n",
      "|road3|       120|     1000|685|  9|AU218| 1.686422178944038E9|       -1|   77| -3|\n",
      "|road3|       120|     1000|762|  9|AU218| 1.686422164655915E9|       -1|   80|  1|\n",
      "|road1|       120|     1000|802|  8|AU372| 1.686422266813969E9|       -1|    0|  0|\n",
      "|road1|       120|     1000|802|  8|AU372| 1.686422245239255E9|       -1|    0|  0|\n",
      "|road1|       120|     1000|802|  8|AU372|1.6864222233056152E9|       -1|    0|  0|\n",
      "|road1|       120|     1000|802|  8|AU372| 1.686422202897731E9|       -1|    0|-85|\n",
      "|road1|       120|     1000|802|  8|AU372| 1.686422202422852E9|       -1|   85| 85|\n",
      "+-----+----------+---------+---+---+-----+--------------------+---------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CALCULATE SPEED AND ACCELERATION\n",
    "\n",
    "# calculate all speeds achieved by each car\n",
    "windowDept = Window.partitionBy(\"plate\").orderBy(col(\"time\").desc())\n",
    "dfCalcs = df.withColumn(\"row\",row_number().over(windowDept))\n",
    "\n",
    "# calc all speeds\n",
    "dfCalcs = dfCalcs.withColumn(\"speed\", F.col(\"x\") - F.lag(\"x\", -1).over(windowDept))\n",
    "\n",
    "# make all values positive\n",
    "dfCalcs = dfCalcs.withColumn(\"speed\", F.abs(F.col(\"speed\")))\n",
    "\n",
    "# calc all accs\n",
    "dfCalcs = dfCalcs.withColumn(\"acc\", F.col(\"speed\") - F.lag(\"speed\", -1).over(windowDept))\n",
    "\n",
    "# drop nulls and row column\n",
    "dfCalcs = dfCalcs.na.drop()\n",
    "dfCalcs = dfCalcs.drop(\"row\")\n",
    "\n",
    "dfCalcs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+----------------+\n",
      "| road|         avg_speed| avg_time_to_cross|total_collisions|\n",
      "+-----+------------------+------------------+----------------+\n",
      "|road2| 35.96995708154506|  27.8009784035318|              96|\n",
      "|road4| 53.53777777777778|18.678399468703304|              61|\n",
      "|road1| 42.98170731707317|23.265711448432402|              59|\n",
      "|road0|32.415929203539825|30.849030849030846|              87|\n",
      "|road3|            60.224|16.604675876726887|              30|\n",
      "+-----+------------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# historical 2\n",
    "\n",
    "# get average speed per road\n",
    "dfStats = dfCalcs.groupBy(\"road\").avg(\"speed\", \"road_size\")\\\n",
    "            .withColumnRenamed(\"avg(speed)\", \"avg_speed\")\\\n",
    "            .withColumnRenamed(\"avg(road_size)\", \"road_size\")\n",
    "\n",
    "# calculate avg time to cross\n",
    "dfStats = dfStats.withColumn(\"avg_time_to_cross\", F.col( \"road_size\") / F.col(\"avg_speed\")).drop(\"road_size\")\n",
    "\n",
    "# get rows where speed = 0 and acc = 0 (collisions)\n",
    "dfCollisions = dfCalcs.filter((F.col(\"speed\") == 0) & (F.col(\"acc\") == 0))\n",
    "\n",
    "# group by road and count\n",
    "dfCollisions = dfCollisions.groupBy(\"road\").count().withColumnRenamed(\"count\", \"total_collisions\")\n",
    "\n",
    "# join the dataframes to get all stats\n",
    "dfStats = dfStats.join(dfCollisions, \"road\", \"left\")\n",
    "\n",
    "dfStats.write.format(\"mongodb\") \\\n",
    "   .mode(\"append\") \\\n",
    "   .option(\"database\", \"roadtracker\") \\\n",
    "   .option(\"collection\", \"historical2\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|plate|total_infractions|\n",
      "+-----+-----------------+\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# historical 3\n",
    "\n",
    "# partition by plate and order by time (twice to have ascending and descending row numbers)\n",
    "windowDept = Window.partitionBy(\"plate\").orderBy(col(\"time\").desc())\n",
    "windowDept2 = Window.partitionBy(\"plate\").orderBy(col(\"time\").asc())\n",
    "\n",
    "# create rows columns\n",
    "dfSpeeds = dfCalcs.withColumn(\"row\",row_number().over(windowDept))\n",
    "dfSpeeds = dfSpeeds.withColumn(\"row2\",row_number().over(windowDept2))\n",
    "\n",
    "# check where speed is greater than 120 and the previous speed was less than road_speed (that is, new infraction)\n",
    "dfSpeeds = dfSpeeds.withColumn(\"change_in_speed\",\n",
    "                   F.when(((F.col(\"speed\") > F.col(\"road_speed\")) & (F.lag(\"speed\", -1).over(windowDept) <= F.lag(\"road_speed\", -1).over(windowDept) )) , 1) \\\n",
    "                   .otherwise(0))\n",
    "\n",
    "# check for vehicles that enter a road with speed > road_speed (infraction)\n",
    "dfSpeeds = dfSpeeds.withColumn(\"change_in_speed\",\n",
    "                     F.when(((F.col(\"speed\") > F.col(\"road_speed\")) & (F.col(\"row2\") ==1)), 1) \\\n",
    "                        .otherwise(F.col(\"change_in_speed\")))\n",
    "\n",
    "# chosen T (change it after testing)\n",
    "t = 2500000000\n",
    "\n",
    "# get all rows where now() - time < t\n",
    "dfSpeeds = dfSpeeds.withColumn(\"past_time\", F.unix_timestamp(F.current_timestamp()).cast(\"double\"))\n",
    "dfSpeeds = dfSpeeds.withColumn(\"diff_time\", F.col(\"past_time\") - F.col(\"time\"))\n",
    "dfSpeeds = dfSpeeds.filter(F.col(\"diff_time\") < t)\n",
    "\n",
    "#  check which cars have more than 10 infractions\n",
    "dfInfractions = dfSpeeds.groupBy(\"plate\").sum(\"change_in_speed\") \\\n",
    "   .withColumnRenamed(\"sum(change_in_speed)\", \"total_infractions\").filter(F.col(\"total_infractions\") >= 10)\n",
    "\n",
    "dfInfractions.write.format(\"mongodb\") \\\n",
    "   .mode(\"append\") \\\n",
    "   .option(\"database\", \"roadtracker\") \\\n",
    "   .option(\"collection\", \"historical3\") \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|plate|\n",
      "+-----+\n",
      "|VQ482|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analise alternativa\n",
    "windowDept = Window.partitionBy(\"plate\").orderBy(col(\"time\").desc())\n",
    "dfCalcs = dfCalcs.withColumn(\"row\",row_number().over(windowDept))\n",
    "\n",
    "# create column \"changed_y\" that is 1 if the car changed y and 0 otherwise\n",
    "dfCalcs = dfCalcs.withColumn(\"changed_lane\",\n",
    "                     F.when(((F.col(\"y\") != F.lag(\"y\", -1).over(windowDept))), 1) \\\n",
    "                        .otherwise(0))\n",
    "\n",
    "# create column \"over_road_speed\" that is 1 if the car is over the road speed and 0 otherwise\n",
    "dfCalcs = dfCalcs.withColumn(\"over_road_speed\",\n",
    "                        F.when(((F.col(\"speed\") > F.col(\"road_speed\"))), 1) \\\n",
    "                        .otherwise(0))\n",
    "\n",
    "# create column \"over_acc\" that is 1 if (acc>40 or acc<-40) and 0 otherwise\n",
    "dfCalcs = dfCalcs.withColumn(\"over_acc\",\n",
    "                        F.when(((F.col(\"acc\") > 40) | (F.col(\"acc\") < -40)), 1) \\\n",
    "                        .otherwise(0))\n",
    "\n",
    "# create a column \"total\" that is the sum of the previous columns for each car\n",
    "# we consider that changing lane 3 times counts as 1 case of unsafe driving\n",
    "dfCalcs = dfCalcs.withColumn(\"total\", F.col(\"changed_lane\") / 3 + F.col(\"over_road_speed\") + F.col(\"over_acc\"))\n",
    "\n",
    "# get all rows where now() - time < t (THIS ISNT WHAT WE WANT, TEMPORARY)\n",
    "#dfCalcs = dfCalcs.withColumn(\"past_time\", F.unix_timestamp(F.current_timestamp()).cast(\"double\"))\n",
    "#dfCalcs = dfCalcs.withColumn(\"diff_time\", F.col(\"past_time\") - F.col(\"time\"))\n",
    "#dfCalcs = dfCalcs.filter(F.col(\"diff_time\") < t)\n",
    "\n",
    "# for each car compute the sum of the column \"total\" and filter the ones that are greater than 4\n",
    "dfCalcs = dfCalcs.groupBy(\"plate\").sum(\"total\").withColumnRenamed(\"sum(total)\", \"total_infractions\").filter(F.col(\"total_infractions\") >= 4)\n",
    "\n",
    "#dfCalcs.collect()\n",
    "dfCalcs.select(\"plate\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
